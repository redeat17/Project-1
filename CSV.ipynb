{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import json\n",
    "#import wordcloud\n",
    "import requests\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from config import api_key\n",
    "import googleapiclient.errors\n",
    "import matplotlib.pyplot as plt\n",
    "import googleapiclient.discovery\n",
    "from youtube_api import YoutubeDataApi\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-12-dd0acf9b2c1f>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-dd0acf9b2c1f>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    with open(code_path) as file:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# List of simple to collect features\n",
    "snippet_features = [\"title\",\n",
    "                    \"publishedAt\",\n",
    "                    \"channelId\",\n",
    "                    \"channelTitle\",\n",
    "                    \"categoryId\"]\n",
    "\n",
    "# Any characters to exclude, generally these are things that become problematic in CSV files\n",
    "unsafe_characters = ['\\n', '\"']\n",
    "\n",
    "# Used to identify columns, currently hardcoded order\n",
    "header = [\"video_id\"] + snippet_features + [\"trending_date\", \"tags\", \"view_count\", \"likes\", \"dislikes\",\n",
    "                                            \"comment_count\", \"thumbnail_link\", \"comments_disabled\",\n",
    "                                            \"ratings_disabled\", \"description\"]\n",
    "\n",
    "\n",
    "#def setup(api_path, code_path):\n",
    "    #with open(api_path, 'r') as file:\n",
    "        #api_key = file.readline()\n",
    "\n",
    "    with open(code_path) as file:\n",
    "        country_codes = [x.rstrip() for x in file]\n",
    "\n",
    "    return api_key, country_codes\n",
    "\n",
    "\n",
    "def prepare_feature(feature):\n",
    "    # Removes any character from the unsafe characters list and surrounds the whole item in quotes\n",
    "    for ch in unsafe_characters:\n",
    "        feature = str(feature).replace(ch, \"\")\n",
    "    return f'\"{feature}\"'\n",
    "\n",
    "\n",
    "def api_request(api_key, country_code):\n",
    "    # Builds the URL and requests the JSON from it\n",
    "    request_url = f\"https://www.googleapis.com/youtube/v3/videos?part=id,statistics,snippet{page_token}chart=mostPopular&regionCode={US}&maxResults=50&key={api_key}\"\n",
    "    request = requests.get(request_url)\n",
    "    if request.status_code == 429:\n",
    "        print(\"Temp-Banned due to excess requests, please wait and continue later\")\n",
    "        sys.exit()\n",
    "    return request.json()\n",
    "\n",
    "\n",
    "def get_tags(tags_list):\n",
    "    # Takes a list of tags, prepares each tag and joins them into a string by the pipe character\n",
    "    return prepare_feature(\"|\".join(tags_list))\n",
    "\n",
    "\n",
    "def get_videos(items):\n",
    "    lines = []\n",
    "    for video in items:\n",
    "        comments_disabled = False\n",
    "        ratings_disabled = False\n",
    "\n",
    "        # We can assume something is wrong with the video if it has no statistics, often this means it has been deleted\n",
    "        # so we can just skip it\n",
    "        if \"statistics\" not in video:\n",
    "            continue\n",
    "\n",
    "        # A full explanation of all of these features can be found on the GitHub page for this project\n",
    "        video_id = prepare_feature(video['id'])\n",
    "\n",
    "        # Snippet and statistics are sub-dicts of video, containing the most useful info\n",
    "        snippet = video['snippet']\n",
    "        statistics = video['statistics']\n",
    "\n",
    "        # This list contains all of the features in snippet that are 1 deep and require no special processing\n",
    "        features = [prepare_feature(snippet.get(feature, \"\")) for feature in snippet_features]\n",
    "\n",
    "        # The following are special case features which require unique processing, or are not within the snippet dict\n",
    "        description = snippet.get(\"description\", \"\")\n",
    "        thumbnail_link = snippet.get(\"thumbnails\", dict()).get(\"default\", dict()).get(\"url\", \"\")\n",
    "        trending_date = time.strftime(\"%y.%d.%m\")\n",
    "        tags = get_tags(snippet.get(\"tags\", [\"[none]\"]))\n",
    "        view_count = statistics.get(\"viewCount\", 0)\n",
    "\n",
    "        # This may be unclear, essentially the way the API works is that if a video has comments or ratings disabled\n",
    "        # then it has no feature for it, thus if they don't exist in the statistics dict we know they are disabled\n",
    "        if 'likeCount' in statistics and 'dislikeCount' in statistics:\n",
    "            likes = statistics['likeCount']\n",
    "            dislikes = statistics['dislikeCount']\n",
    "        else:\n",
    "            ratings_disabled = True\n",
    "            likes = 0\n",
    "            dislikes = 0\n",
    "\n",
    "        if 'commentCount' in statistics:\n",
    "            comment_count = statistics['commentCount']\n",
    "        else:\n",
    "            comments_disabled = True\n",
    "            comment_count = 0\n",
    "\n",
    "        # Compiles all of the various bits of info into one consistently formatted line\n",
    "        line = [video_id] + features + [prepare_feature(x) for x in [trending_date, tags, view_count, likes, dislikes,\n",
    "                                                                       comment_count, thumbnail_link, comments_disabled,\n",
    "                                                                       ratings_disabled, description]]\n",
    "        lines.append(\",\".join(line))\n",
    "    return lines\n",
    "\n",
    "\n",
    "def get_pages(country_code, next_page_token=\"&\"):\n",
    "    country_data = []\n",
    "\n",
    "    # Because the API uses page tokens (which are literally just the same function of numbers everywhere) it is much\n",
    "    # more inconvenient to iterate over pages, but that is what is done here.\n",
    "    while next_page_token is not None:\n",
    "        # A page of data i.e. a list of videos and all needed data\n",
    "        video_data_page = api_request(next_page_token, country_code)\n",
    "\n",
    "        # Get the next page token and build a string which can be injected into the request with it, unless it's None,\n",
    "        # then let the whole thing be None so that the loop ends after this cycle\n",
    "        next_page_token = video_data_page.get(\"nextPageToken\", None)\n",
    "        next_page_token = f\"&pageToken={next_page_token}&\" if next_page_token is not None else next_page_token\n",
    "\n",
    "        # Get all of the items as a list and let get_videos return the needed features\n",
    "        items = video_data_page.get('items', [])\n",
    "        country_data += get_videos(items)\n",
    "\n",
    "    return country_data\n",
    "\n",
    "\n",
    "def write_to_file(country_code, country_data):\n",
    "\n",
    "    print(f\"Writing {country_code} data to file...\")\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(f\"{output_dir}/{time.strftime('%y.%d.%m')}_{country_code}_videos.csv\", \"w+\", encoding='utf-8') as file:\n",
    "        for row in country_data:\n",
    "            file.write(f\"{row}\\n\")\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    for country_code in country_codes:\n",
    "        country_data = [\",\".join(header)] + get_pages(country_code)\n",
    "        write_to_file(country_code, country_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--key_path', help='Path to the file containing the api key, by default will use api_key.txt in the same directory', default='api_key.txt')\n",
    "    parser.add_argument('--country_code_path', help='Path to the file containing the list of country codes to scrape, by default will use country_codes.txt in the same directory', default='country_codes.txt')\n",
    "    parser.add_argument('--output_dir', help='Path to save the outputted files in', default='output/')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    output_dir = args.output_dir\n",
    "    api_key, country_codes = setup(args.key_path, args.country_code_path)\n",
    "\n",
    "    get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6b173a8df31c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msleep\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mui\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSelect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from time import perf_counter\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "sns.set()\n",
    "\n",
    "# Dates info to use for file name later\n",
    "date = datetime.datetime.now() # Gets current date\n",
    "day = date.day # Gets day\n",
    "month = date.month # Gets month\n",
    "year = date.year # Gets year\n",
    "hour = date.hour\n",
    "minute = date.minute\n",
    "if len(str(minute)) == 1: minute = '0'+str(minute)\n",
    "\n",
    "# Function to find the category text\n",
    "def findCat():\n",
    "    showmore = driver.find_element_by_xpath('//*[@id=\"more\"]/yt-formatted-string')\n",
    "    driver.execute_script(\"arguments[0].click();\", showmore)\n",
    "\n",
    "    category = driver.find_element_by_xpath('//*[@id=\"content\"]/yt-formatted-string/a')\n",
    "    return category.get_attribute('innerHTML')\n",
    "# Function to go to a video and find it's category\n",
    "def getCategory(link): \n",
    "    print('========================================')\n",
    "    print('Going to '+link)\n",
    "    driver.get(link) # Go to video\n",
    "    sleep(1) # Wait to load\n",
    "\n",
    "    c = None\n",
    "    try: # Tries to get it\n",
    "        c = findCat()\n",
    "    except: # If it fails, wait some seconds and try again\n",
    "        try:\n",
    "            sleep(3)\n",
    "            c = findCat()\n",
    "        except: \n",
    "            pass\n",
    "    \n",
    "    print(f'Getting categories ({percent(videos.index(link)+1, len(videos))}%)')\n",
    "    return c\n",
    "\n",
    "# Simple function to get percentage\n",
    "def percent(n, total):\n",
    "    return round((n/total)*100, 1)\n",
    "\n",
    "tic = perf_counter()\n",
    "\n",
    "driver = webdriver.Chrome(executable_path = 'tools/chromedriver') # Gets Chrome driver\n",
    "driver.get('https://www.youtube.com/feed/trending') # Go to Youtube Trending page\n",
    "\n",
    "print('Getting links')\n",
    "videos = [thumb.get_attribute('href') for thumb in driver.find_elements_by_id('thumbnail') if thumb.get_attribute('href') != None]\n",
    "print(f'All links listed! ({len(videos)})')\n",
    "\n",
    "categories = [getCategory(link) for link in videos if not None]\n",
    "\n",
    "driver.close()\n",
    "\n",
    "# Create Data Frame\n",
    "print('========================================')\n",
    "print('Making data frame')\n",
    "n = [1 for category in categories]\n",
    "total = sum(n)\n",
    "data = pd.DataFrame({\"Category\": categories, \"n\": n})\n",
    "data = data.groupby('Category')['n'].sum()\n",
    "data = data.to_frame()\n",
    "data.reset_index(level=0, inplace=True)\n",
    "data = data.sort_values(by='n', ascending=True)\n",
    "print(data) \n",
    "\n",
    "# Create chart\n",
    "print('========================================')\n",
    "print('Building chart')\n",
    "mpl.rcParams['font.size'] = 9.0\n",
    "fig1, chart = plt.subplots(constrained_layout=True)\n",
    "\n",
    "cols = [\n",
    "    '#FFDB15', '#3F5E98', '#918E80', '#2F2440', \n",
    "    '#01949A', '#F7BEC0', '#E7625F', '#02894B', \n",
    "    '#EAB996', '#C22660','#CFC1CE', '#9F2B00', \n",
    "    '#729663', '#9E3A14','#ed0000', '#2E3B51'\n",
    "] # Set color palette\n",
    "\n",
    "chart.pie(data['n'], shadow=False, startangle=90, colors=cols)\n",
    "chart.axis('equal')\n",
    "\n",
    "# Sets legend\n",
    "percents = [percent(n, total) for n in data['n']]\n",
    "plt.legend(labels=['%s, %1.1f %%' % (l, s) for l, s in zip(data['Category'], percents)], frameon=False, loc=2, bbox_to_anchor=(.94,.85))\n",
    "\n",
    "centre_circle = plt.Circle((0,0),0.75,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.suptitle(f'\\nYoutube Trending Categories\\n{hour}:{minute} on {day}/{month}/{year}')\n",
    "toc = perf_counter()\n",
    "\n",
    "totalsecs = toc - tic\n",
    "minutes = int(totalsecs//60)\n",
    "fmin = totalsecs/60\n",
    "remain = float(str(fmin-int(fmin))[1:])\n",
    "seconds = int(60*remain)\n",
    "time = f'{minutes} minutes'\n",
    "if seconds != 0: time = time+f' and {seconds} seconds'\n",
    "print('~~~~~~~~~~~~~~~~~~~~')\n",
    "print(f'-~ Done in {time}. ~-')\n",
    "print('~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "\n",
    "# Save figure file\n",
    "fname = f'yt-trending-{day}-{month}-{year}.png'\n",
    "print(f'Saving chart as {fname}')\n",
    "\n",
    "plt.draw()\n",
    "plt.savefig(fname, dpi=700)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    return json.dumps(json_array)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    \"\"\"\n",
    "    Scraper for the trending page on YouTube.\n",
    "    \"\"\"\n",
    "    URL = \"https://www.youtube.{0}\"\n",
    "    @staticmethod\n",
    "    def scrape(country_code=\"com\"):\n",
    "        json_array = []\n",
    "    try:\n",
    "        response = requests.get(Scraper.URL.format(country_code) + \"/feed/trending\")\n",
    "    except ConnectionError as err:\n",
    "            json_array.append({\"error\": str(err)})\n",
    "        return json.dumps(json_array)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        trending_videos = soup.find_all(attrs={\"class\": \"expanded-shelf-content-item\"})\n",
    "        for video_element in trending_videos:\n",
    "            video_obj = dict()\n",
    "            thumbnail = video_element.find(attrs={\"class\": [\"yt-thumb\", \"video-thumb\"]}).find(attrs={\"class\": \"yt-thumb-simple\"}).find(\"img\")\n",
    "            if thumbnail.get(\"data-thumb\") is not None:\n",
    "                    video_obj[\"video_thumbnail\"] = thumbnail[\"data-thumb\"]\n",
    "                else:\n",
    "                    video_obj[\"video_thumbnail\"] = thumbnail[\"src\"]\n",
    "                    title_content = video_element.find(attrs={\"class\": \"yt-lockup-title\"})\n",
    "                    link_meta = title_content.find(\"a\")\n",
    "                    video_obj[\"video_url\"] = link_meta.get(\"href\")\n",
    "                    video_obj[\"video_title\"] = link_meta.get(\"title\")\n",
    "                    video_time = title_content.select(\"span.accessible-description\")\n",
    "                    if len(video_time) != 0:\n",
    "                        video_obj[\"video_time\"] = video_time[0].text\n",
    "                    else:\n",
    "                        video_obj[\"video_time\"] = \"LIVE NOW\"\n",
    "            profile_content = video_element.find(attrs={\"class\": \"yt-lockup-byline\"})\n",
    "            profile_meta = profile_content.find(\"a\")\n",
    "            video_obj[\"profile_url\"] = profile_meta.get(\"href\")\n",
    "            video_obj[\"profile_name\"] = profile_meta.string\n",
    "            meta_info = video_element.find(attrs={\"class\": \"yt-lockup-meta-info\"})\n",
    "            if len(meta_info.contents) > 1:\n",
    "                video_obj[\"upload_date\"] = meta_info.contents[0].string\n",
    "                video_obj[\"view_count\"] = meta_info.contents[1].string.split(\" \")[0]\n",
    "            else:\n",
    "                video_page_response = requests.get(Scraper.URL.format(country_code) + video_obj[\"video_url\"])\n",
    "                parsed_response = BeautifulSoup(video_page_response.text, \"html.parser\")\n",
    "            if parsed_response.find(\"span\", class_=\"date\"):\n",
    "                video_obj[\"upload_date\"] = parsed_response.find(\"span\", class_=\"date\").string\n",
    "            elif parsed_response.find(\"strong\", class_=\"watch-time-text\"):\n",
    "                video_obj[\"upload_date\"] = parsed_response.find(\"strong\", class_=\"watch-time-text\").string\n",
    "                video_obj[\"view_count\"] = meta_info.contents[0].string\n",
    "            description_content = video_element.select(\"div.yt-lockup-description\")\n",
    "            video_description = \"\"\n",
    "            if len(description_content) > 0:\n",
    "                video_description = description_content[0].text\n",
    "                video_obj[\"video_desc\"] = html.escape(str(video_description))\n",
    "                json_array.append(video_obj)\n",
    "return json.dumps(json_array, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
